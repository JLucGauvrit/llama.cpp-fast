# llama.cpp FAST

A fast implementation of the LLaMA model in C++ using ggml.
This repository contains the code and instructions to build and run the model.

## Requirements
- Docker installed on your machine.

